{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f72bc3-7606-4a75-9599-b9899a8e63e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger Train Data:\n",
      "                                                    text  label  intensity\n",
      "id                                                                        \n",
      "10000  How the fu*k! Who the heck! moved my fridge!.....  anger      0.938\n",
      "10001  So my Indian Uber driver just called someone t...  anger      0.896\n",
      "10002  @DPD_UK I asked for my parcel to be delivered ...  anger      0.896\n",
      "10003  so ef whichever butt wipe pulled the fire alar...  anger      0.896\n",
      "10004  Don't join @BTCare they put the phone down on ...  anger      0.896\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fear Train Data:\n",
      "                                                    text label  intensity\n",
      "id                                                                       \n",
      "20000  I feel like I am drowning. #depression #anxiet...  fear      0.979\n",
      "20001  I get so nervous even thinking about talking t...  fear      0.979\n",
      "20002                     I lost my blinders .... #panic  fear      0.975\n",
      "20003  I feel like I am drowning. #depression  #falur...  fear      0.938\n",
      "20004  This is the scariest American Horror Story out...  fear      0.938\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "joy Train Data:\n",
      "                                                    text label  intensity\n",
      "id                                                                       \n",
      "30000  Just got back from seeing @GaryDelaney in Burs...   joy      0.980\n",
      "30001  Oh dear an evening of absolute hilarity I don'...   joy      0.958\n",
      "30002  Been waiting all week for this game â¤ï¸â¤ï...   joy      0.940\n",
      "30003  @gardiner_love : Thank you so much, Gloria! Yo...   joy      0.938\n",
      "30004  I feel so blessed to work with the family that...   joy      0.938\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sad Train Data:\n",
      "                                                    text    label  intensity\n",
      "id                                                                          \n",
      "40000                      Depression sucks! #depression  sadness      0.958\n",
      "40001            Feeling worthless as always #depression  sadness      0.958\n",
      "40002                       Feeling worthless as always   sadness      0.958\n",
      "40003  My #Fibromyalgia has been really bad lately wh...  sadness      0.946\n",
      "40004  Im think ima lay in bed all day and sulk. Life...  sadness      0.934\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = ['id', 'text', 'label', 'intensity']\n",
    "path = \"./\" \n",
    "\n",
    "anger_train = pd.read_csv(path + 'angertraindata.txt', header=None, sep='\\t', names=cols, index_col=0)\n",
    "fear_train = pd.read_csv(path + 'feartraindata.txt', header=None, sep='\\t', names=cols, index_col=0)\n",
    "joy_train = pd.read_csv(path + 'joytraindata.txt', header=None, sep='\\t', names=cols, index_col=0)\n",
    "sad_train = pd.read_csv(path + 'sadtraindata.txt', header=None, sep='\\t', names=cols, index_col=0)\n",
    "\n",
    "print(\"Anger Train Data:\")\n",
    "print(anger_train.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"fear Train Data:\")\n",
    "print(fear_train.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"joy Train Data:\")\n",
    "print(joy_train.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"sad Train Data:\")\n",
    "print(sad_train.head())\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c641d98-c12f-4a1e-91ed-52f519a1de3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Anger Train Data:\n",
      "                                                    text  label  intensity  \\\n",
      "id                                                                           \n",
      "10000  How the fu*k! Who the heck! moved my fridge!.....  anger      0.938   \n",
      "10001  So my Indian Uber driver just called someone t...  anger      0.896   \n",
      "10002  @DPD_UK I asked for my parcel to be delivered ...  anger      0.896   \n",
      "10003  so ef whichever butt wipe pulled the fire alar...  anger      0.896   \n",
      "10004  Don't join @BTCare they put the phone down on ...  anger      0.896   \n",
      "\n",
      "                                              clean_text  \n",
      "id                                                        \n",
      "10000  fu * k ! heck ! moved fridge !... knock landlo...  \n",
      "10001  indian uber driver called someone n word . ' m...  \n",
      "10002  asked parcel delivered pick store address # fu...  \n",
      "10003  ef whichever butt wipe pulled fire alarm davis...  \n",
      "10004  ' join put phone , talk rude . taking money ac...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cleaned Fear Train Data:\n",
      "                                                    text label  intensity  \\\n",
      "id                                                                          \n",
      "20000  I feel like I am drowning. #depression #anxiet...  fear      0.979   \n",
      "20001  I get so nervous even thinking about talking t...  fear      0.979   \n",
      "20002                     I lost my blinders .... #panic  fear      0.975   \n",
      "20003  I feel like I am drowning. #depression  #falur...  fear      0.938   \n",
      "20004  This is the scariest American Horror Story out...  fear      0.938   \n",
      "\n",
      "                                              clean_text  \n",
      "id                                                        \n",
      "20000  feel like drowning . # depression # anxiety # ...  \n",
      "20001  get nervous even thinking talking ****** wanna...  \n",
      "20002                         lost blinders .... # panic  \n",
      "20003  feel like drowning . # depression # falure # w...  \n",
      "20004  scariest american horror story ... ' gonna wat...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cleaned Joy Train Data:\n",
      "                                                    text label  intensity  \\\n",
      "id                                                                          \n",
      "30000  Just got back from seeing @GaryDelaney in Burs...   joy      0.980   \n",
      "30001  Oh dear an evening of absolute hilarity I don'...   joy      0.958   \n",
      "30002  Been waiting all week for this game â¤ï¸â¤ï...   joy      0.940   \n",
      "30003  @gardiner_love : Thank you so much, Gloria! Yo...   joy      0.938   \n",
      "30004  I feel so blessed to work with the family that...   joy      0.938   \n",
      "\n",
      "                                              clean_text  \n",
      "id                                                        \n",
      "30000  got back seeing burslem . amazing !! face stil...  \n",
      "30001  oh dear evening absolute hilarity ' think laug...  \n",
      "30002  waiting week game â ¤ ï ¸ â ¤ ï ¸ â ¤ ï ¸...  \n",
      "30003  : thank much , gloria ! ' sweet , thoughtful !...  \n",
      "30004  feel blessed work family nanny â ¤ ï ¸ nothi...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cleaned Sad Train Data:\n",
      "                                                    text    label  intensity  \\\n",
      "id                                                                             \n",
      "40000                      Depression sucks! #depression  sadness      0.958   \n",
      "40001            Feeling worthless as always #depression  sadness      0.958   \n",
      "40002                       Feeling worthless as always   sadness      0.958   \n",
      "40003  My #Fibromyalgia has been really bad lately wh...  sadness      0.946   \n",
      "40004  Im think ima lay in bed all day and sulk. Life...  sadness      0.934   \n",
      "\n",
      "                                              clean_text  \n",
      "id                                                        \n",
      "40000                    depression sucks ! # depression  \n",
      "40001              feeling worthless always # depression  \n",
      "40002                           feeling worthless always  \n",
      "40003  # fibromyalgia really bad lately good mental s...  \n",
      "40004  im think ima lay bed day sulk . life hitting h...  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "pat3 = r'[0-9]+'\n",
    "combined_pat = r'|'.join((pat1, pat2, pat3))\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    stripped = re.sub(combined_pat, '', text)\n",
    "    lower_case = stripped.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    filtered_words = [w for w in words if not w in stop_words]\n",
    "    return ' '.join(filtered_words).strip()\n",
    "\n",
    "anger_train['clean_text'] = anger_train['text'].apply(tweet_cleaner)\n",
    "fear_train['clean_text'] = fear_train['text'].apply(tweet_cleaner)\n",
    "joy_train['clean_text'] = joy_train['text'].apply(tweet_cleaner)\n",
    "sad_train['clean_text'] = sad_train['text'].apply(tweet_cleaner)\n",
    "\n",
    "print(\"Cleaned Anger Train Data:\")\n",
    "print(anger_train.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Cleaned Fear Train Data:\")\n",
    "print(fear_train.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Cleaned Joy Train Data:\")\n",
    "print(joy_train.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Cleaned Sad Train Data:\")\n",
    "print(sad_train.head())\n",
    "print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd8b826-3c95-4993-98bf-1f6174d72c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Features for Anger Train Data:\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  991  992  993  994  995  996  997  998  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "3  0  0  0  0  0  0  0  0  0  1  ...    0    0    0    0    0    0    0    0   \n",
      "4  0  1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "\n",
      "   999  anger  \n",
      "0    0    NaN  \n",
      "1    0    NaN  \n",
      "2    0    NaN  \n",
      "3    0    NaN  \n",
      "4    0    NaN  \n",
      "\n",
      "[5 rows x 1001 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TF-IDF Features for Anger Train Data:\n",
      "   0  1  2  3  4  5         6  7  8  9  ...  991  992  993  994  995  996  \\\n",
      "0  0  0  0  0  0  0         0  0  0  0  ...    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0  0         0  0  0  0  ...    0    0    0    0    0    0   \n",
      "2  0  0  0  0  0  0         0  0  0  0  ...    0    0    0    0    0    0   \n",
      "3  0  0  0  0  0  0         0  0  0  0  ...    0    0    0    0    0    0   \n",
      "4  0  0  0  0  0  0  0.365734  0  0  0  ...    0    0    0    0    0    0   \n",
      "\n",
      "   997  998  999  anger  \n",
      "0    0    0    0    NaN  \n",
      "1    0    0    0    NaN  \n",
      "2    0    0    0    NaN  \n",
      "3    0    0    0    NaN  \n",
      "4    0    0    0    NaN  \n",
      "\n",
      "[5 rows x 1001 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BoW Features for Fear Train Data:\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  991  992  993  994  995  996  997  998  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "\n",
      "   999  fear  \n",
      "0    0   NaN  \n",
      "1    0   NaN  \n",
      "2    0   NaN  \n",
      "3    0   NaN  \n",
      "4    0   NaN  \n",
      "\n",
      "[5 rows x 1001 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TF-IDF Features for Fear Train Data:\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  991  992  993  994  995  996  997  998  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "\n",
      "   999  fear  \n",
      "0    0   NaN  \n",
      "1    0   NaN  \n",
      "2    0   NaN  \n",
      "3    0   NaN  \n",
      "4    0   NaN  \n",
      "\n",
      "[5 rows x 1001 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BoW Features for Joy Train Data:\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  991  992  993  994  995  996  997  998  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    1    0    0    0    0    0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    1    0    0    0    1    0   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "\n",
      "   999  joy  \n",
      "0    0  NaN  \n",
      "1    0  NaN  \n",
      "2    0  NaN  \n",
      "3    1  NaN  \n",
      "4    0  NaN  \n",
      "\n",
      "[5 rows x 1001 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TF-IDF Features for Joy Train Data:\n",
      "   0  1  2  3         4  5  6  7  8  9  ...  991  992  993  994  995  996  \\\n",
      "0  0  0  0  0         0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0.364856  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
      "2  0  0  0  0         0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
      "3  0  0  0  0         0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
      "4  0  0  0  0         0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
      "\n",
      "        997  998       999  joy  \n",
      "0         0    0         0  NaN  \n",
      "1  0.188822    0         0  NaN  \n",
      "2         0    0         0  NaN  \n",
      "3  0.232484    0  0.414537  NaN  \n",
      "4         0    0         0  NaN  \n",
      "\n",
      "[5 rows x 1001 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BoW Features for Sad Train Data:\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  991  992  993  994  995  996  997  998  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "\n",
      "   999  sadness  \n",
      "0    0      NaN  \n",
      "1    0      NaN  \n",
      "2    0      NaN  \n",
      "3    0      NaN  \n",
      "4    0      NaN  \n",
      "\n",
      "[5 rows x 1001 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TF-IDF Features for Sad Train Data:\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  991  992  993  994  995  996  997  998  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
      "\n",
      "   999  sadness  \n",
      "0    0      NaN  \n",
      "1    0      NaN  \n",
      "2    0      NaN  \n",
      "3    0      NaN  \n",
      "4    0      NaN  \n",
      "\n",
      "[5 rows x 1001 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "anger_labels = pd.get_dummies(anger_train['label'])\n",
    "fear_labels = pd.get_dummies(fear_train['label'])\n",
    "joy_labels = pd.get_dummies(joy_train['label'])\n",
    "sad_labels = pd.get_dummies(sad_train['label'])\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 3))\n",
    "X_BoW_anger = vectorizer.fit_transform(anger_train.clean_text)\n",
    "X_BoW_anger = pd.DataFrame.sparse.from_spmatrix(X_BoW_anger).join(anger_labels)\n",
    "\n",
    "X_BoW_fear = vectorizer.fit_transform(fear_train.clean_text)\n",
    "X_BoW_fear = pd.DataFrame.sparse.from_spmatrix(X_BoW_fear).join(fear_labels)\n",
    "\n",
    "X_BoW_joy = vectorizer.fit_transform(joy_train.clean_text)\n",
    "X_BoW_joy = pd.DataFrame.sparse.from_spmatrix(X_BoW_joy).join(joy_labels)\n",
    "\n",
    "X_BoW_sad = vectorizer.fit_transform(sad_train.clean_text)\n",
    "X_BoW_sad = pd.DataFrame.sparse.from_spmatrix(X_BoW_sad).join(sad_labels)\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf_anger = vectorizer_tfidf.fit_transform(anger_train.clean_text)\n",
    "X_tfidf_anger = pd.DataFrame.sparse.from_spmatrix(X_tfidf_anger).join(anger_labels)\n",
    "\n",
    "X_tfidf_fear = vectorizer_tfidf.fit_transform(fear_train.clean_text)\n",
    "X_tfidf_fear = pd.DataFrame.sparse.from_spmatrix(X_tfidf_fear).join(fear_labels)\n",
    "\n",
    "X_tfidf_joy = vectorizer_tfidf.fit_transform(joy_train.clean_text)\n",
    "X_tfidf_joy = pd.DataFrame.sparse.from_spmatrix(X_tfidf_joy).join(joy_labels)\n",
    "\n",
    "X_tfidf_sad = vectorizer_tfidf.fit_transform(sad_train.clean_text)\n",
    "X_tfidf_sad = pd.DataFrame.sparse.from_spmatrix(X_tfidf_sad).join(sad_labels)\n",
    "\n",
    "print(\"BoW Features for Anger Train Data:\")\n",
    "print(X_BoW_anger.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"TF-IDF Features for Anger Train Data:\")\n",
    "print(X_tfidf_anger.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"BoW Features for Fear Train Data:\")\n",
    "print(X_BoW_fear.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"TF-IDF Features for Fear Train Data:\")\n",
    "print(X_tfidf_fear.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"BoW Features for Joy Train Data:\")\n",
    "print(X_BoW_joy.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"TF-IDF Features for Joy Train Data:\")\n",
    "print(X_tfidf_joy.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"BoW Features for Sad Train Data:\")\n",
    "print(X_BoW_sad.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"TF-IDF Features for Sad Train Data:\")\n",
    "print(X_tfidf_sad.head())\n",
    "print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7235903b-9a11-466c-ac3c-950e24e85081",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joy_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m joy_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mjoy_dev\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(tweet_cleaner)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'joy_dev' is not defined"
     ]
    }
   ],
   "source": [
    "joy_dev['clean_text'] = joy_dev['text'].apply(tweet_cleaner)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
